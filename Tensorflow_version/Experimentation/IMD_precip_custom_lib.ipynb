{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9173717,"sourceType":"datasetVersion","datasetId":5544031},{"sourceId":9173999,"sourceType":"datasetVersion","datasetId":5544214},{"sourceId":9199004,"sourceType":"datasetVersion","datasetId":5561537},{"sourceId":9199047,"sourceType":"datasetVersion","datasetId":5561565},{"sourceId":120202,"sourceType":"modelInstanceVersion","modelInstanceId":101113,"modelId":125305},{"sourceId":121254,"sourceType":"modelInstanceVersion","modelInstanceId":101440,"modelId":125634}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Flatten, Dense, Conv2D, MaxPooling2D,GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\nfrom IPython.display import Image\nimport cv2 as cv\nimport os\nimport pydot\nfrom tensorflow.keras.layers import Dropout\nfrom skimage.metrics import peak_signal_noise_ratio ,structural_similarity,mean_squared_error\nimport pickle\nimport sys\nsys.path.append('/kaggle/input/imd_custom_library/other/default/7')\nfrom Data_Processing import Data_Processing as dp \nfrom Data_Processing import Training as tn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-26T11:51:45.205398Z","iopub.execute_input":"2024-09-26T11:51:45.205689Z","iopub.status.idle":"2024-09-26T11:51:58.278036Z","shell.execute_reply.started":"2024-09-26T11:51:45.205656Z","shell.execute_reply":"2024-09-26T11:51:58.277184Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Data Processing and Visualisation","metadata":{}},{"cell_type":"code","source":"SR_file_path = '/kaggle/input/imd-gridded-vanilla/data/data.pkl'\nwith open(SR_file_path, 'rb') as file:\n    SR_data = pickle.load(file)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T11:51:58.279751Z","iopub.execute_input":"2024-09-26T11:51:58.280766Z","iopub.status.idle":"2024-09-26T11:53:03.842705Z","shell.execute_reply.started":"2024-09-26T11:51:58.280719Z","shell.execute_reply":"2024-09-26T11:53:03.841714Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"SR_data_base,Metadata = dp.process_base_data(SR_data)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T11:53:03.843865Z","iopub.execute_input":"2024-09-26T11:53:03.844185Z","iopub.status.idle":"2024-09-26T11:53:06.092756Z","shell.execute_reply.started":"2024-09-26T11:53:03.844150Z","shell.execute_reply":"2024-09-26T11:53:06.091976Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"SR_data_processed,LR_data_processed,Metadata,daily_max = dp.generate_dataset(SR_data_base,Metadata)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T11:53:06.094776Z","iopub.execute_input":"2024-09-26T11:53:06.095107Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Step 1 completed\nStep 2 completed\n","output_type":"stream"}]},{"cell_type":"code","source":"SR_mask,LR_mask = dp.create_mask([SR_data_base[0]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dp.visualize_LR_masked_data(LR_data_processed[0],LR_mask[0])\ndp.visualize_SR_data(SR_data_processed[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generator","metadata":{}},{"cell_type":"code","source":"class DepthToSpaceLayer(tf.keras.layers.Layer):\n    def __init__(self, block_size, **kwargs):\n        super(DepthToSpaceLayer, self).__init__(**kwargs)\n        self.block_size = block_size\n\n    def call(self, inputs):\n        return tf.nn.depth_to_space(inputs, self.block_size)\ndef residual_block_gen(ch=64, k_s=3, st=1):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Conv2D(ch, k_s, strides=(st, st), padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.LeakyReLU(),\n        tf.keras.layers.Conv2D(ch, k_s, strides=(st, st), padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.LeakyReLU(),\n    ])\n    return model\ndef Upsample_block(x, ch=256, k_s=3, st=1):\n    x = tf.keras.layers.Conv2D(ch, k_s, strides=(st, st), padding='same')(x)\n    x = DepthToSpaceLayer(block_size=2)(x)  \n    x = tf.keras.layers.LeakyReLU()(x)\n    return x\ninput_lr = tf.keras.layers.Input(shape=(None, None, 1))\ninput_conv = tf.keras.layers.Conv2D(64, 3\n                                    , padding='same')(input_lr)\ninput_conv = tf.keras.layers.LeakyReLU()(input_conv)\nGenerator = input_conv\nfor _ in range(8):\n    res_output = residual_block_gen()(Generator)\n    Generator = tf.keras.layers.Add()([Generator, res_output])\nGenerator = tf.keras.layers.Conv2D(64, 3, padding='same')(Generator)\nGenerator = tf.keras.layers.BatchNormalization()(Generator)\n\nGenerator = tf.keras.layers.Add()([Generator, input_conv])\n\nGenerator = Upsample_block(Generator)  \nGenerator = Upsample_block(Generator) \noutput_sr = tf.keras.layers.Conv2D(1, 3, activation='sigmoid', padding='same')(Generator)\n\nGenerator = tf.keras.models.Model(input_lr, output_sr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Discriminator","metadata":{}},{"cell_type":"code","source":"def residual_block_disc(ch=64, k_s=3, st=1):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Conv2D(ch, k_s, strides=(st, st), padding='same',\n                               kernel_initializer=tf.keras.initializers.HeNormal()),\n        tf.keras.layers.LeakyReLU(alpha=0.2),\n        # Optional: Move BatchNormalization after activation or remove\n        # tf.keras.layers.BatchNormalization(),\n    ])\n    return model\n\ninput_lr = tf.keras.layers.Input(shape=(128, 128, 1))\ninput_conv = tf.keras.layers.Conv2D(64, 3, padding='same', kernel_initializer=tf.keras.initializers.HeNormal())(input_lr)\ninput_conv = tf.keras.layers.LeakyReLU(alpha=0.2)(input_conv)\n\nchannel_nums = [64, 128, 128, 256, 256, 512, 512]\nstride_sizes = [2, 1, 2, 1, 2, 1, 2]\n\ndisc = input_conv\nfor x in range(7):\n    disc = residual_block_disc(ch=channel_nums[x], st=stride_sizes[x])(disc)\n\ndisc = tf.keras.layers.Flatten()(disc)\n#disc = tf.keras.layers.Droput(0.1)(disc)\ndisc = tf.keras.layers.Dense(1024, kernel_initializer=tf.keras.initializers.HeNormal())(disc)\ndisc = tf.keras.layers.LeakyReLU(alpha=0.2)(disc)\ndisc_output = tf.keras.layers.Dense(1)(disc) \ndiscriminator = tf.keras.models.Model(input_lr, disc_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Extractor","metadata":{}},{"cell_type":"code","source":"from keras.models import load_model\ndef my_custom_loss(y_true, y_pred):\n    ssim_loss = 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))\n    mse_loss = tf.reduce_mean(tf.square(y_true - y_pred))\n    return 0.9 * ssim_loss + 0.1* mse_loss\nmodel = load_model('/kaggle/input/autoencoder_fe_2509/tensorflow2/default/1/autoencoder_fe.keras',custom_objects={'combined_loss': my_custom_loss})\nfeature_extractor = Model(inputs=model.input, outputs=model.get_layer('latent_space').output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"PSNR = []\nSSIM = []\nMSE = []\n\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.0, beta_2=0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(5e-4, beta_1=0.0, beta_2=0.5)\n\ndef train_step(SR_images, LR_images, batch_size):\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        fake_images = Generator(LR_images, training=True)\n        real_output = discriminator(SR_images, training=True)\n        fake_output = discriminator(fake_images, training=True)\n        gp = tn.gradient_penalty(discriminator, SR_images, fake_images)\n        gen_loss = tn.generator_loss(fake_output, SR_images, fake_images, feature_extractor)\n        disc_loss = tn.discriminator_loss(real_output, fake_output, gp)\n    \n    # Compute gradients for the generator, discriminator\n    gradients_of_generator = gen_tape.gradient(gen_loss, Generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n    \n    # Apply gradients to update the weights\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, Generator.trainable_variables ))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n    \n    return gen_loss, disc_loss\n\n# Training loop\ndef train(SR_data, LR_data,PSNR,SSIM, epochs, batch_size):\n    for epoch in range(epochs):\n        indices = np.arange(len(SR_data))\n        np.random.shuffle(indices)\n        for i in range(len(SR_data)//batch_size):\n            batch = indices[i:i+batch_size]\n            lr = np.array([LR_data[j] for j in batch]).reshape((len(batch_size), 32, 32, 1))\n            sr = np.array([SR_data[j] for j in batch]).reshape((len(batch_size), 128, 128, 1))\n            gen_loss, disc_loss = train_step(sr, lr, len(batch_indices))\n            if i%20==0:\n                print(f'{i} batches completed in epoch:{epoch}')\n        print(f'Epoch {epoch+1}, Generator Loss: {gen_loss.numpy()}, Discriminator Loss: {disc_loss.numpy()}')\n        psnr,ssim , mse = tn.calculate_metrics(Generator,SR_data,LR_data)\n        print(f'PSNR:{psnr},SSIM:{ssim} , MSE:{mse}')\n        PSNR.append(psnr)\n        SSIM.append(ssim)\n        MSE.append(mse)\n        sr_pic = np.array(SR_data[0])\n        lr_pic = np.array(LR_data[:32]).reshape((32, 32, 32, 1))\n        generated = np.array(Generator(lr_pic))[0].reshape((128,128,1))\n        dp.visualize_SR_masked_data(sr_pic,SR_mask)\n        dp.visualize_SR_masked_data(generated,SR_mask)\n        Generator.save(f'generator_epoch_{epoch+1}.h5')\n        discriminator.save(f'discriminator_epoch_{epoch+1}.h5')\n        print(f'Models saved after epoch {epoch+1}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tn.plot_metrics(PSNR,SSIM,MSE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}